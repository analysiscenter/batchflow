{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces you to the modelling features of BatchFlow library via creating a simple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from batchflow.models.tf import TFModel\n",
    "from batchflow.models.torch import TorchModel\n",
    "from batchflow import Dataset, C, V, F, B, action, Batch\n",
    "from batchflow.models.metrics import ClassificationMetrics\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a batch class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a specific batch class is not required, though convenient. Besides, here it helps you get an idea of what batch components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatch(Batch):\n",
    "    \"\"\" Batch class for regression models \"\"\"\n",
    "    components = 'features', 'labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the batches will have 2 components: features and labels. Batch components might be thought of as columns in a table, while batch items are rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Firstly, we consider a linear regression that allows solving tasks where targets are continuous variables.\n",
    "\n",
    "For this reason we generate data from uniform or normal distributions, multiply it by normally distributed weights and add normally distributed noise and then try to predict it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(size, dist='unif', shape=13):\n",
    "    \"\"\" Generation of data to fit linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        data length\n",
    "\n",
    "    dist: {'unif', 'norm'}\n",
    "        sample distribution 'unif' or 'norm'. Default is 'unif'\n",
    "    \n",
    "    shape: int\n",
    "        a length of a feature vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        Uniformly or normally distributed array\n",
    "\n",
    "    y: numpy array\n",
    "        array with some random noize\n",
    "    \"\"\"\n",
    "    if dist == 'unif':\n",
    "        x = np.random.uniform(0, 2, size=(size, shape))\n",
    "    elif dist == 'norm':\n",
    "        x = np.random.normal(size=(size, shape))\n",
    "\n",
    "    w = np.random.normal(loc=1., size=(shape, 1))\n",
    "    error = np.random.normal(loc=0., scale=0.1, size=(size, 1))\n",
    "\n",
    "    y = np.dot(x, w) + error\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `x` and `y` are numpy arrays:\n",
    "- `x` is a matrix with __size__ rows and 13 columns\n",
    "- `y` is a vector of __size__ items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "linear_x, linear_y = generate_linear_data(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create a dataset (an instance of Dataset class) which generates batches of __MyBatch__ class. \n",
    "\n",
    "Even though the dataset does not have any data yet, it contains the full index of dataset items. So we can split it into __train__ and __test__ parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dset = Dataset(size, batch_class=MyBatch)\n",
    "linear_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation the dataset is empty, until data is loaded with [a pipeline](https://analysiscenter.github.io/batchflow/intro/pipeline.html) which allows you to use and perform action-methods from the [batch class](https://analysiscenter.github.io/batchflow/intro/batch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (linear_dset.train.p\n",
    "                       .load(src=(linear_x, linear_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline above only loads data. Clearly, it doesn't train a linear regression, therefore it is not enough for us.   \n",
    "Hence, we need to create a linear regression model.   \n",
    "1.    One way to do this is to write your own class.   \n",
    "In case of TF models you need to inherit from [TFModel](https://analysiscenter.github.io/batchflow/api/batchflow.models.tf.base.html#batchflow.models.tf.base.TFModel) base class, in case Pytorch models - [TorchModel](https://analysiscenter.github.io/batchflow/api/batchflow.models.torch.base.html#batchflow.models.torch.base.TorchModel)      \n",
    "Define `initial_block`, `body` or `head` methods, which are the parts your model consist of, inside your class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of linear model for Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class TFLinearModel(TFModel):\n",
    "    @classmethod\n",
    "    def body(cls, inputs, units, name='body', **kwargs):\n",
    "        with tf.variable_scope(name):\n",
    "            return tf.layers.dense(inputs, units=units, name='dense')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class TorchLinearModel(TorchModel):\n",
    "    @classmethod\n",
    "    def body(cls, inputs, units, **kwargs):\n",
    "        in_features = np.prod(inputs.shape[1:])\n",
    "        return nn.Linear(in_features, units)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on how to create your own model see [tf](https://analysiscenter.github.io/batchflow/intro/tf_models#how-to-write-a-custom-model) and [torch](https://analysiscenter.github.io/batchflow/intro/torch_models.html#how-to-write-a-custom-model) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Another way to do this is to declare your model through the [config](https://analysiscenter.github.io/batchflow/intro/torch_models.html#how-to-configure-a-model). \n",
    "\n",
    "Batchflow provides you with simple and convinient interface for creating deep neural networks with just a few lines of code.   \n",
    "The configs for the same TF and Torch models are identical.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all model config should contain __inputs__ sections for input tensors (placeholders) parameters:\n",
    "- shape\n",
    "- tensor's name\n",
    "- typical transformations (like one-hot-encoding)\n",
    "and so on.\n",
    "\n",
    "To configure 'inputs' use the __inputs_config__ dict shown in the cell below. This dict has two keys:\n",
    "* __features__ - the name of the placeholder for the input data.\n",
    "* __labels__ - the name of the placeholder for the answers before all transformations.\n",
    "\n",
    "Values for these keys are dicts themselves which describe `features` and `labels` placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': B.features.shape[1:],\n",
    "    'labels/shape': B.labels.shape[1:]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter B in _values_ is a [named expression](https://analysiscenter.github.io/batchflow/intro/named_expr.html) which replaces the name within it into a value from a batch class attribute or a component.   \n",
    "In this case, it is the [component](https://analysiscenter.github.io/batchflow/intro/batch.html#components) name defined in the batch class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'features',\n",
    "    'body/layout': 'f',\n",
    "    'body/units': 1,\n",
    "    'loss': 'mse',\n",
    "    'optimizer': 'Adam',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config keys:\n",
    "* __inputs__ - input data configuration (described above).\n",
    "* __initial_block/inputs__ - the name of input tensor which should be fed into the model.\n",
    "* __body/layout__ - the sequence of layers in the body of your network. `f` stands for the single fully connected layer.\n",
    "* __body/units__ - number of neurons in fully-connected layers. Can be a _list_, if you have more than one dense layer.\n",
    "* __loss__ - a loss function to optimize.\n",
    "* __optimizer__ - an optimization algorithm and its parameters.\n",
    "\n",
    "As you can see, some configuration options have a hierarchical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is ready, you need to train it. The pipeline allows you to do this with just two functions:\n",
    "* [__init_model__](https://analysiscenter.github.io/batchflow/api/batchflow.pipeline.html?highlight=init_model#batchflow.Pipeline.init_model) allows to initialize the model with given config.\n",
    "* [__train_model__](https://analysiscenter.github.io/batchflow/api/batchflow.pipeline.html?highlight=train_model#batchflow.Pipeline.train_model) [fits a model with training data](https://analysiscenter.github.io/batchflow/intro/models.html#training-a-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a pipeline, which can generate batches and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind that you can train TF or Torch models with the same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    'model': TFModel \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    'model': TorchModel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we exploit Torch models. Just link the pipeline config above with the model to the pipeline itself.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_linear = (linear_dset.train.pipeline(pipeline_config) \n",
    "                .load(src=(linear_x, linear_y))\n",
    "                .init_variable('loss', [])\n",
    "                .init_model('dynamic',\n",
    "                            C('model'),\n",
    "                            name='linear',\n",
    "                            config=model_config)\n",
    "                .train_model('linear', \n",
    "                             features=B('features'),\n",
    "                             labels=B('labels'),\n",
    "                             fetches='loss',\n",
    "                             save_to=V('loss', mode='a'))\n",
    "                .run_later(BATCH_SIZE, shuffle=True, n_epochs=300, bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 165/2400 [00:05<15:00,  2.48it/s] "
     ]
    }
   ],
   "source": [
    "train_linear.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_linear.v('loss')\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction pipeline would be also helpful. For this purpose the method [__predict_model__](https://analysiscenter.github.io/batchflow/intro/models.html#predicting-with-a-model) is used. \n",
    "\n",
    "I would direct your attention to the argument named __fetches__.\n",
    "It returns a value of tensor with a specified name. Using it, you can always get from model any tensor you want.\n",
    "\n",
    "As you might already know, in __output__ configuration parameter (when calling __init_model__) you can specify a list of useful outputs, such as 'proba', 'sigmoid', etc. (if you don't know about it, read [the documentation](https://analysiscenter.github.io/batchflow/intro/tf_models#output)). And __predict_model__'s __fetches__ argument allows to get those outputs from a model.\n",
    "\n",
    "Another important method is [__import_model__](https://analysiscenter.github.io/batchflow/intro/models.html#importing-models) that loads a model from another pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linear = (linear_dset.test.p\n",
    "                 .load(src=(linear_x, linear_y))\n",
    "                 .import_model('linear', train_linear)\n",
    "                 .init_variable('predict', [])\n",
    "                 .predict_model('linear',\n",
    "                                fetches='predictions',\n",
    "                                features=B('features'),    \n",
    "                                targets=B('labels'),\n",
    "                                save_to=V('predict', mode='a'))\n",
    "                 .run(BATCH_SIZE, shuffle=False, n_epochs=1, bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last __pipeline__, we test our model. Let's see, how well does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = np.array(test_linear.get_variable('predict')).reshape(-1, 1)\n",
    "target = np.array(linear_y[linear_dset.test.indices])\n",
    "\n",
    "error = np.mean(np.abs((target - predict) * 100 / target))\n",
    "\n",
    "print('Average error: {}%'.format(round(error, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is far from being perfect because the training was too short (in order not make you wait too long). Increase the number of epochs in the training pipeline up to 1000 to get a more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression \n",
    "It solves a task with a binary target (0 or 1, -1 or 1 and etc).\n",
    "\n",
    "To train a logistic regression we generate two-dimensional data from two linearly separable clusters and fit a model to predict a cluster for a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logistic_data(size, first_params, second_params):\n",
    "    \"\"\" Generation of data for fit logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        number of data items\n",
    "\n",
    "    first_params: list of list\n",
    "        distribution params for cloud #0\n",
    "\n",
    "    second_params: list of list\n",
    "        distribution params for cloud #1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        coordinates in two-dimensional space\n",
    "\n",
    "    y: numpy array\n",
    "        labels {0, 1}\n",
    "    \"\"\"\n",
    "    first = np.random.multivariate_normal(first_params[0], first_params[1], size)\n",
    "    second = np.random.multivariate_normal(second_params[0], second_params[1], size)\n",
    "\n",
    "    x = np.vstack((first, second))\n",
    "    y = np.hstack((np.zeros(size), np.ones(size)))\n",
    "    shuffle = np.arange(len(x))\n",
    "    np.random.shuffle(shuffle)\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle] #.reshape(-1, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500\n",
    "logistic_x, logistic_y = generate_logistic_data(size, [[1,2],[[15,0],[0,15]]], [[10,17],[[15,0],[0,15]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(logistic_x[:,0], logistic_x[:,1], c=logistic_y)\n",
    "plt.title('Cloud points distribution', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important things that you need to know is that it really doesn't matter which model you want to train and what data you will use for it. The procedure stays the same.\n",
    "\n",
    "First of all, a dataset is created and split into train/test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_dset = Dataset(size, batch_class=MyBatch)\n",
    "logistic_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pipeline and configurations also does not change much:\n",
    "\n",
    "- labels __classes__ - the number of classes. We have a binary classification, hence 2 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': B.features.shape[1:],\n",
    "    'labels/classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model configuration for logistic regression. The difference is that now we need 2 output neurons(one for each class) and [CrossEntropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'features',\n",
    "    'body': {'layout': 'f', 'units': 2},\n",
    "    'loss': 'ce',\n",
    "    'optimizer': 'Adam',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and execute the pipeline with __run__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_logistic = (logistic_dset.train.pipeline(pipeline_config)\n",
    "                .load(src=(logistic_x, logistic_y))\n",
    "                .init_variable('loss', [])\n",
    "                .init_model('dynamic',\n",
    "                            C('model'),\n",
    "                            'logistic',\n",
    "                            config=model_config)\n",
    "                .train_model('logistic',\n",
    "                             fetches='loss',\n",
    "                             features=B('features'),\n",
    "                             labels=B('labels'),\n",
    "                            save_to=V('loss', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_logistic.v('loss')\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, create test pipeline and run it too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logistic = (logistic_dset.test.p\n",
    "                .import_model('logistic', train_logistic)\n",
    "                .load(src=(logistic_x, logistic_y))\n",
    "                .init_variables(['predictions', 'metrics'])\n",
    "                .predict_model('logistic', \n",
    "                             fetches='predictions' ,\n",
    "                             features=B('features'),\n",
    "                             targets=B('labels'),\n",
    "                             save_to=V('predictions'))\n",
    "                .gather_metrics(ClassificationMetrics, targets=B('labels'), predictions=V('predictions'),\n",
    "                                fmt='logits', axis=-1, save_to=V('metrics', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=False, n_epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After measure the quality of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_logistic.get_variable('metrics').evaluate('accuracy')\n",
    "print('Percentage of accurate predictions: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again to imporove the accuracy increase the number of epochs in the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson regression \n",
    "[Poisson regression](https://en.wikipedia.org/wiki/Poisson_regression) is being used if the answers contain counts. \n",
    "\n",
    "The example shows how we can train poisson regression by using data generated from poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(lam, size=10, shape=13):\n",
    "    \"\"\" Generation of data for fit poisson regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        number of data items\n",
    "\n",
    "    lam : float\n",
    "        Poisson distribution parameter\n",
    "    \n",
    "    shape : int\n",
    "        number of features for each data item\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy array\n",
    "        Matrix with random numbers from the uniform distribution\n",
    "    y: numpy array\n",
    "        random Poisson distributed numbers\n",
    "    \"\"\"\n",
    "    x = np.random.random(size=(size, shape))\n",
    "    b = np.random.random(1)\n",
    "\n",
    "    y_obs = np.random.poisson(np.exp(np.dot(x, lam) + b))\n",
    "\n",
    "    shuffle = np.arange(len(x))\n",
    "    np.random.shuffle(shuffle)\n",
    "    x = x[shuffle]\n",
    "    y = y_obs[shuffle].reshape(-1, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "NUM_DIM = 13\n",
    "poisson_x, poisson_y = generate_poisson_data(np.random.random(NUM_DIM), size, NUM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the same cell as previously, but with different names of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_dset = Dataset(size, batch_class=MyBatch)\n",
    "poisson_dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the model's input tensors would be infered directly from the data in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'features/shape': B.features.shape[1:],\n",
    "    'labels/shape': B.labels.shape[1:]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again define two configs, one for TF model, another - for Pytorch. They differ on the 3 keys which must be implemented using the corresponding framerowk:    \n",
    "* __model__ - the base class for your model.\n",
    "* __loss__ - loss function.\n",
    "* __output__ - operation to be applied to the output tensor from your network.\n",
    "\n",
    "We store the frameworks oriented things in the `pipeline_config`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, set config for TF.   \n",
    "Define custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_poisson_loss(target, predictions):\n",
    "    loss = tf.reduce_mean(tf.nn.log_poisson_loss(target, predictions))\n",
    "    tf.losses.add_loss(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF model pipeline config.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    'model': TFModel,\n",
    "    'loss': tf_poisson_loss,\n",
    "    'output': tf.exp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline config for Pytorch model.\n",
    "Note that in case Pytorch Poisson loss is already being [implemented](https://pytorch.org/docs/stable/nn.html#poissonnllloss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    'model': TorchModel,\n",
    "    'loss': nn.PoissonNLLLoss(),\n",
    "    'output': torch.exp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of the configs above, they do essentially the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model config for Poisson regression.      \n",
    "Letter `C` here in is another named expression, [config option](https://analysiscenter.github.io/batchflow/intro/named_expr.html#c-config-option)   \n",
    "As soon as pipeline config would be linked to the pipeline itself, all the expressions `C('name')` inside the pipeline as well as the model config would be replaced with the corresponding values from the pipeline config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'features',\n",
    "    'body': {'layout': 'f', 'units': 1},\n",
    "    'loss': C('loss'),\n",
    "    'optimizer': 'Adam',\n",
    "    'output': C('output'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run a train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_poisson = (poisson_dset.train.pipeline(pipeline_config)\n",
    "                .load(src=(poisson_x, poisson_y))\n",
    "                .init_variable('loss', [])\n",
    "                .init_model('dynamic', \n",
    "                            C('model'), \n",
    "                            'poisson',\n",
    "                            config=model_config)\n",
    "                .train_model('poisson',\n",
    "                             fetches='loss',\n",
    "                             features=B('features'),\n",
    "                             targets=B('labels'),\n",
    "                             save_to=V('loss', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=100, bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_poisson.v('loss')\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test pipeline and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poisson = (poisson_dset.test.p\n",
    "                .load(src=(poisson_x, poisson_y))\n",
    "                .import_model('poisson', train_poisson)\n",
    "                .init_variable('predictions', [])\n",
    "                .predict_model('poisson', \n",
    "                               fetches='exp',\n",
    "                               features= B('features'),\n",
    "                               targets=B('labels'),\n",
    "                               save_to=V('predictions', mode='a'))\n",
    "                .run(BATCH_SIZE, shuffle=True, n_epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.array(test_poisson.get_variable('predictions')).reshape(-1, 1)\n",
    "target = np.array(poisson_y[poisson_dset.test.indices])\n",
    "\n",
    "true_var = np.mean((target - np.mean(target))**2)\n",
    "predict_var = np.mean((pred - np.mean(pred))**2)\n",
    "\n",
    "error = np.mean(np.abs(pred - target)) / np.mean(target) * 100\n",
    "print('Average error: {}%'.format(round(error, 3)), 'Variance ratio: %.3f' % (predict_var / true_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* No matter what you want to train and what data you want to use for it, pipeline always looks the same.\n",
    "* It takes time to train an accurate model.\n",
    "* Now you know how to use `batchflow` and specifically how to:\n",
    "    * create pipelines for train and test models\n",
    "    * train linear regression with multi-dimensional data\n",
    "    * train logistic regression (with another type of data - two-dimensions clouds of dots)\n",
    "    * create your own loss function and train poisson regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "You might hone your new skills by:\n",
    "* creating a multi-class regression model;\n",
    "* adding aditional features to the model in order to improve the quality.\n",
    "\n",
    "You might also want to dig deeper into [batch operations](./02_batch_operations.ipynb).\n",
    "\n",
    "Or choose another topic from [the table of contents](./00_content.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
