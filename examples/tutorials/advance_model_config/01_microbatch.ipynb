{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the point of microbathing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, bigger batch size allows for better gradients estimates, thus helping training models. Unfortunately, there is a hard constraint on device memory: even the modern accelerators can't fit more that hundreds of gigabytes, which is sometimes just not enough. This is where `microbatch` comes to the rescue: it allows to evenly split batch data into multiple pieces (called microbatches), evaluate gradients on each of them separately, and the apply average value of gradient directly to the weights of the model.\n",
    "\n",
    "Parameter `microbatch` allows us to leverage simple trade-off between bigger batch size (thus model performance) and model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from batchflow import Pipeline, B, C, V, D\n",
    "from batchflow.opensets import Imagenette320\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which GPU(s) to be used. More about it in [CUDA documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset, define a default model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataset = Imagenette320(bar=True)\n",
    "\n",
    "model_config = {'inputs/images/shape': B.image_shape,\n",
    "                'inputs/labels/classes': D.num_classes,\n",
    "                'initial_block/inputs': 'images'}\n",
    "\n",
    "BATCH_SIZE = 4160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model without microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (Pipeline()\n",
    "                  .init_variable('loss_history', [])\n",
    "                  .init_model('dynamic', ResNet18, 'conv_nn', config=model_config)\n",
    "                  .resize((320, 320))\n",
    "                  .to_array()\n",
    "                  .train_model('conv_nn', fetches='loss',\n",
    "                               images=B.images, labels=B.labels,\n",
    "                               save_to=V('loss_history', mode='a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = train_template << dataset.train\n",
    "try:\n",
    "    train_pipeline.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)\n",
    "except tf.errors.ResourceExhaustedError:\n",
    "    print('ResourceExhaustedError')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get **ResourceExhaustedError** because batch didn't fit into GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add `microbatch` to model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({'microbatch': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, batches will be split into microbatches with size 64 inside the [TFModel.train](https://analysiscenter.github.io/batchflow/api/batchflow.models.tf.base.html#batchflow.models.tf.base.TFModel.train) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **MICROBATH SIZE MUST BE A DIVISOR OF THE BATCH SIZE!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_microbatch = (Pipeline()\n",
    "                       .init_variable('loss_history', [])\n",
    "                       .init_model('dynamic', ResNet18, 'conv_nn', config=model_config)\n",
    "                       .resize((320, 320))\n",
    "                       .to_array()\n",
    "                       .train_model('conv_nn', fetches='loss',\n",
    "                                    images=B.images, labels=B.labels,\n",
    "                                    save_to=V('loss_history', mode='a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:06<00:00, 45.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f54606817b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_microbatch = template_microbatch << dataset.train\n",
    "pipeline_microbatch.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn’t have `microbatch` in the model configuration and can also pass `microbatch` parameter to the `train_model` action of pipeline to dynamically change splitting strategy. You could run pipeline with parameter `train_model` parameter `microbatch=microbatch_size`:\n",
    "```python\n",
    "train_model('conv_nn', fetches='loss',\n",
    "            microbatch=64,\n",
    "            images=B.images, labels=B.labels,\n",
    "            save_to=V('loss_history', mode='a'))\n",
    "```\n",
    "If you defined `microbatch`  in both places (model configuration and train model action), then value in `train_model` will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **If the data in batch can fit memory constraints, there is no reason to use `microbatch` due to inherently slower processing of batches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train models using `microbatch` and you might want to see next tutorial about [multiple devices](./02_device.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
